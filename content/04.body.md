
## Proposal Body (2000 words)

* Base enabling technologies:
  * Low dimensional representations (Elana)

Latent space determination relies on computational techniques that learn low dimensional structure from high dimensional data. A wide variety of computational methods for dimensionality reduction have been applied to single cell data. These techniques fall largely into two classes: (1) linear techniques based upon matrix factorization and (2) non-linear techniques based upon manifold learning. Each specific technique selected for analysis will reveal distinct structures in the data. Much of the current research emphasizes demonstration of optimality of a single method. In contrast to this prevaling view, we posit that each method reveals a unique set of features associated with a set of *a priori* unknown biological processes. Together, the distribution of the ensemble of low dimensional representations from these methods will uncover the multiple, concurrent biological processes from high dimensional datasets. Previously, we have developed a common language for interpretation of matrix factorization methods to facilitate a unification of latent space methods [@doi:10.1016/j.tig.2018.07.003]. The latent space team in the previous round of funding (containing PIs Fertig, Goff, Greene, and Patro) is also developing standardized output formats for these distinct methods. Together, this previous work will facilitate practical unification and biological assessment of latent spaces in this seed network. 

Numerous factors impact the biological accuracy of latent spaces learned from dimension reduction techniques. Two notable examples include the preprocessing of the input data (Aim 1) and dimensionality of the low dimensional representation (Aim 2). The impact of each of these components on inference must be quantified and isolated before latent methods can be compared or unified. To that end, we select a single dimension reduction technique as the foundation for the technologies developed in our seed network. We use the Bayesian, non-negative matrix factorization method scCoGAPS [@doi:10.1101/378950,@doi:10.1101/395004] (PI Fertig). Previous work has demonstrated the robustness of this approach in inferring biologically relevant low dimensional representations of perturbation [@doi:10.1186/1471-2164-13-160,@doi:10.18632/oncotarget.12075] and time course [@doi:10.1007/978-1-62703-721-1_6,@doi:10.1186/s13073-018-0545-2] from bulk datasets, leading to the winning solution in the HPN DREAM8 challenge [@doi:10.1038/nmeth.3773]. Moreover, recent extension of this method to single cell data in the developing mouse retina distinguished simultaneously cellular identity, dynamic trajectories, and cell state within a single analysis [@doi:10.1101/378950]. Notably for the work proposed in this seed network, this method includes an uncertainty estimate that can be readily modified to account for measurement-specific technical variation [@doi:10.1371/journal.pone.0078127]. Moreover, this same study demonstrated that low dimensional factorizations of tumor populations separates tumor samples relative to normals whereas higher dimensionalities separate tumor subtypes. Thus, CoGAPS has been shown to yield different factorizations reflective of biological hierarchies at different dimensionalities, rather than having a single dimensionality with a "correct" factorization. While technology development will focus on scCoGAPS, the tools developed as part of the seed network will be generalizable and compared to additional linear and non-linear methods developed in the literature **ADD RELEVANT CITATIONS** and by members of the broader CZI consortium. This single-method focus with subsequent expansion to additional methods will also facilitate educational initiatives to advance broader understanding of the interpretation of latent space solutions from high dimensional data (Aim 3).

  * Fast & improved quantification  (Rob / Mike)

The technologies to improve quantification will have a critical impact on the outcomes of latent spaces. However, there are currently no standardized, quantatative metrics to determine relative uncovery of biology from low dimensional representations. We have developed new transfer learning methods to quantify the extent to which latent space representations from one set of training data are represented in another [@doi:10.1101/395004,@10.1101/395947]. These tools provide a strong foundation to enable biological quantifaction of latent space representations by quantifying the extent to which those spaces transfer across datasets of related biological contexts.    


### Aim 1

* Fast search: (in low dimensions, quantifying differences between case and reference maps, twist: shared latent spaces / k-mers)
  * Differences b/w maps (Stephanie)
  * New models for UMI deduplication accounting for transcript-level information (Rob)
    * Parsimony & likelihood based, integrated with gene-level uncertainty
  * Everything FAST! API for search against HCA reference? (Rob)
  * k-mer / quantified latent spaces (Casey / Rob)

### Aim 2

* Eschewing marker genes: Practical exploration of the HCA in latent spaces
  * Search tool for perturbations / signatures in latent-space(s) (Loyal)
    - Differential analysis of latent space usage across contexts
  * Latent space transformations for progression? Consider jawns for semi-supervised learning? Dimensionality estimation? (Elana)
  * Transfer learning of signatures _between/across_ tissues (Loyal)

* Reference Cell types
  * Cell-type summarized expression profiles (Mike, Loyal)
    - A 'reference catalog' of reduced dimensional representations
  * Versioning & provenance of cell types / features as the reference dataset changes (Mike)
  * 'Power-user' application of latent-space discovery in novel dataset and projection of HCA into new learned spaces.

### Aim 3

* Delivery
  * Training / teaching (scRNAseq, low-dimensional representations, RFA-developed tools) (Tom)
  * Software hardening/testing (Casey - software eng)
  * Bioconductor integration (Stephanie, Mike)
